上一章，我们介绍了给nlp领域带来巨大变革以及使用注意力机制搭建的模型Transformer
针对Encoder、Decoder两个部分不同的特点，开始出现不同的针对Transformer的优化思路
**针对Encoder层**：
	Google仅选择了Encoder层，通过**将Encoder层进行堆叠**，**再提出不同的预训练-掩码语言模型（Masked Language Model,MLM)**,**打造了一统自然语言理解任务代表模型**---BERT
针对Decoder层：
	  OpenAI则选择了Decoder层，使用原有的语言模型（LM）任务，通过不断增加模型参数和预训练语料，打造了在NLG（Natural Language Generation)任务上优势明显的GPT系列模型，也是现今大火的LLM的基座模型
当然还有一种思路，保留Encoder和Decoder：
		打造预训练的Transformer模型，例如Google发布的T5模型
		
接下来三个篇章（Encoder-Only、Encoder-Decoder、Decoder-Only的顺序来依次介绍Transformer时代的各个主流预训练模型，分别介绍三种核心的模型架构、每种主流模型选择的预训练任务及其独特优势，这也是目前所有主流 LLM 的模型基础。

---
### 3.1.1 BERT
该模型发布于论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》，
实现了包括 GLUE、MultiNLI 等七个自然语言处理评测任务的最优性能（State Of The Art，**SOTA**），
自BERT推出以来，**预训练+微调的模式**开始成为自然语言处理任务的主流。
不仅 **BERT 自身在不断更新迭代提升模型性能**，也出现了如 MacBERT、BART 等**基于 BERT 进行优化提升的模型。**

#### （1）思想继承
- Transformer架构
- 预训练+微调范式

#### (2）模型架构

BERT是针对NLU任务打造的预训练模型，其输入一般是文本序列，而输出一般是label
Eg.例如情感分类的积极、消极的label。
但是正如Transformer是一个Seq2Seq模型，使用Encoder堆叠而成的BERT本质上也是一个Seq2Seq模型，只是没有加入特定的Decoder，
因此，为适配各种NLU任务，在模型的最顶层加入了一个分类头prediction_heads，用于将**多维度的隐藏状态通过线性层转换到分类维度**
（Eg.如果一共有两个类别，prediction_heads输出的就是一个二维向量）

模型整体就是由Embedding、Encoder加上prediction_heads组成：
![[Pasted image 20250827101542.png]]
输出的文本序列通过tokenizer转化为input_ids（基本上每一个模型在tokenier的操作都类似）然后进入Embedding层转化为特定维度的hidden_states，再经过Encoder块。Encoder块中是堆叠起来的n层Encoder Layer ，
BERT有两种规模的模型，
分别是base版本（12层 Encoder Layer，768 的隐藏层维度，总参数量 110M），
large 版本（24层 Encoder Layer，1024 的隐藏层维度，总参数量 340M）。
**通过Encoder编码之后的最顶层hidden_states最后经过prediction_heads 就得到了最后的类别概率，经过softmax计算就可以计算出模型预测的类别**
> BERT 采用 WordPiece 作为分词方法。
> WordPiece 是一种基于统计的子词切分算法，其核心在于将**单词拆解为子词**（例如，"playing" -> ["play", "##ing"]）。
> 其合并操作的依据是**最大化语言模型的似然度**。对于中文等非空格分隔的语言，通常将单个汉字作为原子分词单位（token）处理。

**prediction_heads其实就是线性层加上激活函数，一般而言，最后一个线性层的输出维度和任务的类别数相同**
![[Pasted image 20250827103038.png]]
而每一层 Encoder Layer 都是和 Transformer 中的 Encoder Layer 结构类似的层
![[Pasted image 20250827103054.png]]
**已经通过Embedding层映射的hidden_states进入核心的attention机制，然后通过残差连接的机制和原输入相加，再经过一层intermediate层得到最终输出。**
**intermediate层是BERT的特殊称呼，其实就是一个线性层加上激活函数**
![[Pasted image 20250827103317.png]]
（注意，BERT所使用的激活函数是GELU函数，全名为高斯误差线性单元激活函数，这也是自 BERT 才开始被普遍关注的激活函数。）
GELU的计算公式：
![[Pasted image 20250827103544.png]]
GELU的核心思路为**将随机正则的思想引入激活函数，通过输入自身的概率分布，来决定抛弃还是保留自身的神经元。**

BERT 的 注意力机制和 Transformer 中 Encoder 的 自注意力机制几乎完全一致，
但是 **BERT 将相对位置编码融合在了注意力机制中，将相对位置编码同样视为可训练的权重参数**
![[Pasted image 20250827103814.png]]

如图，BERT的注意力计算过程和Transformer的唯一差异在于，**在完成注意力分数的计算之后，先通过Position Embedding融入相对位置信息。**
这里的Position Embedding层，其实就是一层线性矩阵。
通过**可训练的参数来拟合相对位置**，相对而言比Transformer使用的绝对位置编码Sinusoidal**能够拟合更丰富的相对位置信息，**
但是，这样也**增加了不少模型参数，同时完全无法处理超过模型训练长度的输入**
（Eg.对 BERT 而言能处理的最大上下文长度是 512 个 token）

可以看出，BERT 的模型架构既是建立在 Transformer 的 Encoder 之上的，这也是为什么说 BERT 沿承了 Transformer 的思想.

----
### （3）预训练任务--MLM+NSP
相较于基本沿承Transformer的模型架构，
BERT更大的创新点在于其提出的两个新的预训练任务上——**MLM 和 NSP（Next Sentence Prediction，下一句预测）**
预训练-微调范式的核心优势在于，**通过将预训练和微调分离**，完成一次完成一次预训练的模型可以仅通过微调应用在几乎所有下游任务上，只要微调的成本较低，**即使预训练成本是之前的数倍甚至数十倍，模型仍然有更大的应用价值。**
因此，可以进一步**扩大模型参数和预训练数据量**，使用海量的预训练语料来让模型拟合潜在语义和底层知识，从而让模型通过长时间、大规模的预训练获得强大的语言理解和生成能力。

因此，预训练数据的核心要求即是**需要极大的数据规模（数亿token）**。毫无疑问，通过人工标注产出的全监督数据很难达到这个规模。
因此，**预训练数据一定是从无监督的语料中获取。**
这也就是为什么传统的预训练任务都是LM的原因--LM使用上文预测下文的方式可以直接应用到任何文本中，对于任意文本，我们只需要**将下文mask 将上文输入模型要求其预测就可以实现LM训练**。
因此互联网上所有文本语料都可以被用于预训练。

但是，LM预训练任务的一大缺陷在于，其**直接拟合从左到右的语义关系，但忽略了双向的语义关系**。
虽然 Transformer 中通过位置编码表征了文本序列中的位置信息，但这和直接拟合双向语义关系还是有本质区别。
(例如，BiLSTM（双向 LSTM 模型）在语义表征上就往往优于 LSTM 模型，就是因为 BiLSTM 通过双向的 LSTM 拟合了双向语义关系。)

基于这一思想，Jacob 等学者提出了 MLM，也就是掩码语言模型作为新的预训练任务。相较于模拟人类写作的 LM，MLM 模拟的是“完形填空”。

```
输入：I <MASK> you because you are <MASK>
输出：<MASK> - love; <MASK> - wonderful
```
通过这样的任务，模型可以拟合双向语义，也就能更好地实现文本的理解。
同样，**MLM 任务无需对文本进行任何人为的标注，只需要对文本进行随机遮蔽即可，因此也可以利用互联网所有文本语料实现预训练。** 例如，BERT 的预训练就使用了足足 3300M 单词的语料。

缺陷：
LM模拟了人的创作过程，其训练和下游任务是完全一致的，也就是说训练时是通过上文预测下文，下游任务微调和推理也同样如此。
**MLM不同，在下游任务微调和推理时，其实是不存在我们人工加入的 **<mask>,**我们会通过原文本得到对应的隐藏状态再根据下游任务进入分类器或其它组件 **。
预训练和微调的不一致，会极大程度影响模型在下游任务微调的性能。针对这一问题，作者对 MLM 的策略进行了改进。

在具体进行 MLM 训练时，会随机选择训练语料中 15% 的 token 用于遮蔽。但是这 15% 的 token 并非全部被遮蔽为 `<MASK>`，而是有 80% 的概率被遮蔽，10% 的概率被替换为任意一个 token，还有 10% 的概率保持不变。
**其中 10% 保持不变就是为了消除预训练和微调的不一致，而 10% 的随机替换核心意义在于迫使模型保持对上下文信息的学习。**
因为如果全部遮蔽的话，模型仅需要处理被遮蔽的位置，从而仅学习要预测的 token 而丢失了对上下文的学习。
通过引入部分随机 token，模型无法确定需要预测的 token，从而被迫保持每一个 token 的上下文表征分布，从而具备了对句子的特征表示能力。且由于随机 token 的概率很低，其并不会影响模型实质的语言理解能力。

除去 MLM，BERT 还提出了另外一个预训练任务——NSP，即下一个句子预测。
NSP 的核心思想是针对句级的 NLU 任务，例如问答匹配、自然语言推理等。问答匹配是指，输入一个问题和若干个回答，要求模型找出问题的真正回答；
自然语言推理是指，输入一个前提和一个推理，判断推理是否是符合前提的。这样的任务都需要模型在**句级去拟合关系**，判断两个句子之间的关系，而不仅是 MLM 在 token 级拟合的语义关**系。**
**因此，BERT 提出了 NSP 任务来训练模型在句级的语义关系拟合**。

**NSP 任务的核心思路是要求模型判断一个句对的两个句子是否是连续的上下文**

无限量的训练数据：
 ** 同样，由于 NSP 的正样本可以从无监督语料中随机抽取任意连续的句子，而负样本可以对句子打乱后随机抽取（只需要保证不要抽取到原本就连续的句子就行），因此也可以具有几乎无限量的训练数据。 ** 


-----
### (4) 下游任务微调
这种思想的关键就在于如何低成本的微调快速迁移到对应的下游任务上。

针对这一点，BERT设计了更通用的输入和输出层来适配多任务下的迁移学习。
对每一个输入的文本序列，BERT会在**其首部加入一个特殊的token <cls>**
在**后续的编码中，该token代表的即是整句的状态，也就是句级的语义表征。**
在进行NSP预训练时，就使用了**该token对应的特征向量来作为最后分类器的输入**

在完成预训练后，针对每一个下游任务，只需要使用一定量的全监督人工标注数，对预训练的 BERT 在该任务上进行微调即可。
所谓微调，其实和训练时更新模型参数的策略一致，只不过在**特定的任务、更少的训练数据、更小的 batch_size 上**进行训练，更新参数的幅度更小。
对于绝大部分下游任务，都可以直接使用BERT的输出。
（Eg.对于文本分类任务，直接直接修改最后模型结构中的prediction_heads最后的分类头即可。
   对于序列标注等任务，可以集成 BERT 多层的隐含层向量再输出最后的标注结果。
   对于文本生成任务，也同样可以取 Encoder 的输出直接解码得到最终生成结果。)


-----
### 3.1.2 RoBERTa
在传统深度学习的范式中，对每一个任务，我们需要从0训练一个模型，那么久无法使用太大的模型参数，否则需要极大规模的有监督数据才能让模型较好地拟合，成本太大。

但在预训练-微调范式，我们在预训练阶段可以使用尽可能大量的训练数据，只需要一次预训练好的模型，后续在每一个下游任务上通过少量有监督数据微调即可。而 BERT 就使用了 13GB（3.3B token）的数据进行预训练，这相较于传统 NLP 来说是一个极其巨大的数据规模了。

#### (1) 优化一 ：去掉NSP预训练任务
RoBERTa 的模型架构与 BERT 完全一致，也就是使用了 BERT-large（24层 Encoder Layer，1024 的隐藏层维度，总参数量 340M）的模型参数。在预训练任务上，有学者质疑 NSP 任务并不能提高模型性能，因为其太过简单，加入到预训练中并不能使下游任务微调时明显受益，甚至会带来负面效果。RoBERTa 设置了四个实验组：

```
1. 段落构建的 MLM + NSP：BERT 原始预训练任务，输入是一对片段，每个片段包括多个句子，来构造 NSP 任务；
2. 文档对构建的 MLM + NSP：一个输入构建一对句子，通过增大 batch 来和原始输入达到 token 等同；
3. 跨越文档的 MLM：去掉 NSP 任务，一个输入为从一个或多个文档中连续采样的完整句子，为使输入达到最大长度（512），可能一个输入会包括多个文档；
4. 单文档的 MLM：去掉 NSP 任务，且限制一个输入只能从一个文档中采样，同样通过增大 batch 来和原始输入达到 token 等同
```

实验结果证明，后两组显著优于前两组，且单文档的 MLM 组在下游任务上微调时性能最佳。
因此，RoBERTa 在预训练中去掉了 NSP，只使用 MLM 任务。

同时，RoBERTa 对 MLM 任务本身也做出了改进。
在 BERT 中，Mask 的操作是在数据处理的阶段完成的，因此后期预训练时同一个 sample 待预测的 `<MASK>` 总是一致的。
由于 BERT 共训练了 40 个 Epoch，为使模型的训练数据更加广泛，BERT 将数据进行了四次随机 Mask，也就是每 10个 Epoch 模型训练的数据是完全一致的。

而 RoBERTa 将 Mask 操作放到了训练阶段，也就是动态遮蔽策略，从而让每一个 Epoch 的训练数据 Mask 的位置都不一致。
在实验中，动态遮蔽仅有很微弱的优势优于静态遮蔽，但由于动态遮蔽更高效、易于实现，后续 MLM 任务基本都使用了动态遮蔽。

#### (2) 优化二：更大规模的预训练数据和预训练步长
#### (3) 优化三： 更大的bpe词表
与 BERT 使用的 WordPiece 算法不同，RoBERTa 使用了 BPE 作为 Tokenizer 的编码策略。BPE，即 Byte Pair Encoding，字节对编码，是指以子词对作为分词的单位。
（Eg.如，对“Hello World”这句话，可能会切分为“Hel，lo，Wor，ld”四个子词对。而对于以字为基本单位的中文，一般会按照字节编码进行切分。例如，在 UTF-8 编码中，“我”会被编码为“E68891”，那么在 BPE 中可能就会切分成“E68”，“891”两个字词对。）

一般来说，BPE编码的词典越大，编码效果越好。
当然，**由于Embedding层就是把token从词典空间映射到隐藏空间**，越大的词表也会带来模型参数的增加。

通过上述三个部分的优化，RoBERTa 成功地在 BERT 架构的基础上刷新了多个下游任务的 SOTA，也一度成为 BERT 系模型最热门的预训练模型。同时，RoBERTa 的成功也证明了更大的预训练数据、更大的预训练步长的重要意义，这也是 LLM 诞生的基础之一。


-----
### 3.1.3 ALBERT
在 BERT 的基础上，RoBERTa 进一步探究了更大规模预训练的作用。
同样是基于 BERT 架构进行优化的 ALBERT 模型，则从是否能够减小模型参数保持模型能力的角度展开了探究。
通过对模型结构进行优化并对 NSP 预训练任务进行改进，ALBERT 成功地以更小规模的参数实现了超越 BERT 的能力。
虽然 ALBERT 所提出的一些改进思想并没有在后续研究中被广泛采用，但其降低模型参数的方法及提出的新预训练任务 SOP 仍然对 NLP 领域提供了重要的参考意义。

#### (1)优化一 ：将Embedding参数进行分解
####   (2) 优化二： 跨层进行参数共享
在具体实现上，其实就是 ALBERT 仅初始化了一个 Encoder 层。在计算过程中，仍然会进行 24次计算，但是每一次计算都是经过这一个 Encoder 层。

不足:
减小了模型参数量并且还提高了模型效果
ALBERT 的参数量远小于 BERT，但训练效率却只略微优于 BERT，因为在模型的设置中，虽然各层共享权重，但计算时仍然要通过 24次 Encoder Layer 的计算，也就是说训练和推理时的速度相较 BERT 还会更慢。这也是 ALBERT 最终没能取代 BERT 的一个重要原因。

####   (3)优化三 : 提出SOP预训练任务
不同于 RoBERTa 选择直接去掉 NSP，ALBERT 选择改进 NSP，增加其难度，来优化模型的预训练。

在传统的 NSP 任务中，正例是由两个连续句子组成的句对，而负例则是从任意两篇文档中抽取出的句对，模型可以较容易地判断正负例，并不能很好地学习深度语义。
而 SOP 任务提出的改进是，正例同样由两个连续句子组成，但负例是将这两个的顺序反过来。
也就是说，模型不仅要拟合两个句子之间的关系，更要学习其顺序关系，这样就大大提升了预训练的难度。例如，相较于我们在上文中提出的 NSP 任务的示例，SOP 任务的示例形如：
```
输入：
    Sentence A：I love you.
    Sentence B: Because you are wonderful.
输出：
    1（正样本）

输入：
    Sentence A：Because you are wonderful.
    Sentence B: I love you.
输出：
    0（负样本）
```
结果：
使用 MLM + SOP 预训练的模型效果优于仅使用 MLM 预训练的模型更优于使用 MLM + NSP 预训练的模型。

作为预训练时代的 NLP 王者，BERT 及 BERT 系模型在多个 NLP 任务上扮演了极其重要的角色。
除去上文介绍过的 RoBERTa、ALBERT 外，还有许多从其他更高角度对 BERT 进行优化的后起之秀，包括进一步改进了预训练任务的 ERNIE、对 BERT 进行蒸馏的小模型 DistilBERT、主打多语言任务的 XLM 等，本文就不再一一赘述。
以 BERT 为代表的 Encoder-Only 架构并非 Transformer 的唯一变种，接下来，我们将介绍 Transformer 的另一种主流架构，与原始 Transformer 更相似、以 T5 为代表的 Encoder-Decoder 架构。

