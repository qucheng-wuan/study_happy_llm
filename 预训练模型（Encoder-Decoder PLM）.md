## 3.2.1 T5
(Text-To-Text Transfer Transformer) 是由Google提出的一种预训练语言模型，通过所有NLP任务统一表示为**文本到文本**的转换问题，大大简化了模型设计和任务处理。
T5 基于Transformer架构，包括编码器和解码器两个部分，使用自注意力机制和多头注意力捕捉全局依赖关系，利用相对位置编码处理长序列中的位置信息，并在每层中包含前馈神经网络进一步处理特征。

T5 的大一统思想将不同的 NLP 任务如文本分类、问答、翻译等统一表示为**输入文本到输出文本的转换**。
好处很大，不仅减少了任务特定的模型调试工作，还能够使用相同的数据处理和训练框架，极大地提升了多任务学习的性能和应用的便捷性。
下面我们从模型结构、预训练任务和大统一思想三个方面来介绍T5模型：

----
### (1) 模型结构: Encoder- Decoder
T5采用了Encoder-Decoder结构，其中编码器和解码器都是基于Transformer架构设计。
编码器用于处理输入文本，解码器用于生成输出文本。
编码器和解码器之间通过注意力机制进行信息交互，
从而实现输入文本到输出文本的转换。
![[Pasted image 20250901193437.png]]
整体来看T5模型结构包括Tokenier部分和Transformer部分。
Tokenier部分主要负责将输入文本转换为模型可接受的输入格式，包括分词，编码等操作。
Transformer 部分又分为 EncoderLayers 和 DecoderLayers 两部分，他们分别由一个个小的 Block组成，每个 Block 包含了多头注意力机制、前馈神经网络和 Norm 层。

其实像encoder和decoder都是基于Transformer来设计的 所以大差不差。
主要包括Self-attention和前馈神经网络
Self-attention用于**捕捉输入序列中的全局依赖关系**，
前馈神经网络用于**处理特征的非线性变换**。

有个好玩的不同点是，在Decoder中还包含Encoder-Decoder Attention结构，用于**捕捉输入和输出序列之间的依赖关系。**
这两种attention结构几乎完全一致，只有在位置编码和mask机制上有所不同
![[Pasted image 20250901194251.png]]
这种attention机制都是差不多的。
Self-attention机制是一种全局依赖关系建模方法，通过计算Query，key,Value之间的相似度来捕捉输入序列中的全局依赖关系。
Encoder-Decoder Attention仅仅在位置编码和Mask机制上有所不同，主要是为了区分输入和输出序列。

与原始的Transformer模型不同，T5模型的LayerNorm采用了RMSNorm，通过**计算每个神经元的均方根来归一化每个隐藏层的激活值。**
RMSNorm的参数设置与LayerNormalization相比更简单，只有一个可学参数，可以更好地适应不同的任务和数据集

这种归一化有助于通过确保权重的规模不会变得过大或过小来稳定学习过程，这在具有许多层的深度学习模型中特别有用。

----
### （2） 预训练任务
我们可以简单概括一下 T5 的预训练任务，主要包括以下几个部分：

- **预训练任务**: T5模型的预训练任务是 MLM，也称为BERT-style目标。具体来说，就是在输入文本中随机遮蔽15%的token，然后让模型预测这些被遮蔽的token。这个过程不需要标签，可以在大量未标注的文本上进行。
- **输入格式**: 预训练时，T5将输入文本转换为"文本到文本"的格式。对于一个给定的文本序列，随机选择一些token进行遮蔽，并用特殊的占位符(token)替换。然后将被遮蔽的token序列作为模型的输出目标。
- **预训练数据集**: T5 使用了自己创建的大规模数据集"Colossal Clean Crawled Corpus"(C4)，该数据集从Common Crawl中提取了大量干净的英语文本。C4数据集经过了一定的清洗，去除了无意义的文本、重复文本等。
- **多任务预训练**: T5 还尝试了将多个任务混合在一起进行预训练，而不仅仅是单独的MLM任务。这有助于模型学习更通用的语言表示。
- **预训练到微调的转换**: 预训练完成后，T5模型会在下游任务上进行微调。微调时，模型在任务特定的数据集上进行训练，并根据任务调整解码策略。

----
### （3）大一统思想
何为大一统思想 很奇怪吧
这个就是把所有的NLP任务都可以统一为文本到文本的任务，这一思想在自然语言处理领域具有深远影响。
其设计理念是将所有不同类型的NLP任务（如文本分类、翻译、文本生成、问答等）转换为一个统一的格式：输入和输出都是纯文本。

例如：
- 对于文本分类任务，输入可以是“classify: 这是一个很好的产品”，输出是“正面”；
- 对于翻译任务，输入可以是“translate English to French: How are you?”, 输出是“Comment ça va?”。

T5通过大规模的文本数据进行预训练，然后再具体任务上进行微调。这一过程与BERT、GPT等模型类似，但T5将预训练和微调阶段的任务统一为文本到文本的形式，使其在各种任务上适应性更强。
像BERT是转化为标签 GPT是前缀 然后文本续写。

有个要注意的点，**对于NLP任务，每次输入前都会加上一个任务描述前缀，明确指定当前任务的类型。这不仅帮助模型在预训练阶段学习到不同任务之间的通用特征，也便于微调阶段迅速适应具体任务**。（例如，任务前缀可以是“summarize: ”用于摘要任务，或“translate English to German: ”用于翻译任务。)
