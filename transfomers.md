## 2.1 注意力机制

- 全连接神经网络（FNN）  ---即每一层的神经元都和上下两层的每一个神经元完全连接
- 卷积神经网络（CNN）    ---即训练参数量远小于全连接神经网络的卷积层来进行特征提取和学习
- 循环神经网络（RNN）      ---能够使用历史信息作为输入，包含环和自重复的网络

由于NLP任务所需要处理的文本通常是序列，因此专用于处理序列、时序数据的RNN往往能够在NLP任务上取得最优的效果。
事实上，在注意力机制横空出世之前，RNN 以及 RNN 的衍生架构 LSTM 是 NLP 领域当之无愧的霸主。（例如，我们在第一章讲到过的开创了预训练思想的文本表示模型 ELMo，就是使用的双向 LSTM 作为网络架构。）

但RNN及LSTM虽然具有捕捉时序信息、适合序列生成的优点，却有两个难以弥补的缺陷：
1. 序列依序计算的模式能够很好地模拟时序信息，==但限制了计算机并行计算的能力==。由于序列需要依次输入、依序计算，图形处理器（Graphics Processing Unit，GPU）并行计算的能力受到了极大限制，导致 RNN 为基础架构的模型虽然参数量不算特别大，但计算时间成本却很高；
    
2. RNN 难以捕捉长序列的相关关系。在 RNN 架构中，==距离越远的输入之间的关系就越难被捕捉，同时 RNN 需要将整个序列读入内存依次计算，也限制了序列的长度==。虽然 LSTM 中通过门机制对此进行了一定优化，但对于较远距离相关关系的捕捉，RNN 依旧是不如人意的。

针对这样的问题，Vaswani 等学者参考了在 CV 领域被提出、被经常融入到 RNN 中使用的注意力机制（Attention），创造性的搭建了完全由注意力机制构成的神经网络----Transformer，也就是llm的鼻祖及核心架构，从而让注意力机制一跃成为深度学习最核心的架构之一。

但是你知道何为attention机制吗？
其实很好理解，核心思想就是当我们关注一张图片，我们往往无需看清楚全部内容而仅将注意力集中在重点部分即可。而在自然语言处理领域，我们往往也可以通过将重点注意力集中在一个或几个token，从而取得更高效高质的计算结果。

attention机制有三个核心变量：
**==Query(查询值)、key（键值）和value（真值==）**。我们可以通过一个案例来理解每一个变量所代表的含义。例如，当我们有一篇新闻报道，我们想要找到这个报道的时间。

但是，如果我们想要匹配的 Query 是一个包含多个 Key 的概念呢？
（例如，我们想要查找“fruit”，此时，我们应该将 apple 和 banana 都匹配到，但不能匹配到 chair。因此，我们往往会选择将 Key 对应的 Value 进行组合得到最终的 Value。）

---
## 2.1.2 深入理解注意力机制

第一章里面有了解到词向量 
合理的训练拟合可以让语义相近的词在向量空间的距离更近，相反，语义较远的词在向量空间的距离更远。
**欧式距离**  ---衡量向量的相似性
同样，我们也可以用**点积**来衡量相似性
$$
v*w = \sum_{i} v_{i}w_{i}
$$
其实我觉得很好理解了，根据词向量的定义，**语义相近的两个词对应的向量的点积该大于0**（++）
那么我们就可以点积来计算词之间的相似度。
-Query 为“fruit”，对应的词向量为 q
-我们的 Key 对应的词向量为 k=[Vapple Vbanana Vchair]
然后可以计算Query和每一个key的相似程度：
![[Pasted image 20250825194621.png]]
K为将所有key对应的词向量堆叠形成的矩阵。 T是代表转置的意思
我们得到的x反应了Query和每一个key的相似程度，
我们再通过一个**softmax层将其转化为和为1的权重**
![[Pasted image 20250825195327.png]]
这样，就可以反映Query和每一个key之间的相似程度，
同时又相加权重为 1，
也就是我们的注意力分数 。在这里举一个例子：
（因为 Softmax 后是 **所有分数相加为 1**，而 **单个分数是 “该 Key 被 Query 关注的比重”** 。比如 3 个 Key，Softmax 后可能是 `[0.6, 0.3, 0.1]`，分别代表给这 3 个 Key 分配 60%、30%、10% 的注意力，**总和是 1** ）
最后，我们再将得到的注意力分数和值向量(value)做对应乘积即可。
根据上述过程，我们就可以得到注意力机制计算的基本公式:
![[Pasted image 20250825200446.png]]
目前，我们离标准的注意力机制还差最后一步。
![[Pasted image 20250825200816.png]]


----
## 2.1.3 注意力机制的实现

下面 用pytorch基本实现下注意力机制的代码
```
import torch

import math

  

def attention(query, key, value, dropout=None):

    '''

    我刚开始在想这个dropout=None的作用 然后搜索后发现有作用：

    1.明确的关闭信号

    用 None 作为默认值，

    比直接设 dropout=0 更清晰地表达 “默认不使用 dropout” 的意图。

    他就不会按概率丢弃部分权重

    2.留扩展接口

    当需要启动dropout时,可以很方便的设置比如attention(x,y,z,dropout=0.5)

    query:查询值

    key:关键值

    value:真值

    '''

    #获取键向量的维度，键向量的维度和值向量的维度相同

    d_k = query.size(-1)

    #"-1"表示最后一个维度

    scores = torch.matmul(query,key.transpose(-2,-1)) / math.sqrt(d_k)

    '''

    key是一个张量,形状通常为 (batch_size, num_heads, seq_len_k, d_k)

    (seq_len_k是键序列长度,d_k是每个键向量的维度)

    交换 key 的倒数第二个维度和最后一个维度，(d_k, seq_len_k)

    转置的目的是为了让 query 和 key 满足矩阵乘法的维度要求

    （矩阵乘法要求前一个矩阵的列数等于后一个矩阵的行数）。

    '''

    #计算Q与K的内积并处以根号dk

    #transpose--相当于转置

    p_attn =scores.softmax(dim=-1)

    '''

    但每个查询向量(seq_len_q 维度)

    对应的一行元素会被归一化到 [0,1] 区间，且每行总和为 1。

    '''

    if dropout is not None:

        p_attn = dropout(p_attn)

    #dropout前面默认None了

    #其实应用dropout正则化 就是防止过拟合

    return torch.matmul(p_attn,value),p_attn

    #输出加权求和后的注意力输出（用于后续模型计算）
```
## 2.1.4  自注意力
但是，在我们的实际应用中，我们往往只需要计算Query和Key之间的注意力结果，很少存在额外的真值Value。也就是说，我们其实只需要拟合两个文本序列。
在**经典的注意力机制中，Q来自于一个序列，K和V来自于另一个序列，** 都通过参数矩阵计算得到，从而拟合这两个序列之间的关系。
（例如Transformer的Decoder的结构中，Q来自于Decoder的输入，K和V来自于Encoder的输出，从而拟合了编码信息与历史信息之间的关系，便于综合这两种）

但在Transformer的Encoder结构中，使用的是 注意力机制的变种 -- 自注意力（self-attention)
其实自注意力我觉得很好从字面上来理解，即是计算本身序列中每个元素对其它元素的注意力分布，
即在计算过程1中1，Q,K,V都由同一个输入通过不同的参数矩阵计算得到。在Encoder中，
我觉得有一个很好的例子 我就拿”我爱吃苹果“来举例
```
1.输入每一个token（我 / 爱 / 吃 / 苹果）先转成嵌入向量`x1~x4`；
2.用 Wq、Wk、Wv 分别得到每个 token 的 Q、K、V 向量（q1~q4、k1~k4、v1~v4）；
3.计算注意力分数：比如 “我” 的 q1 会和所有 token 的 k（k1、k2、k3、k4）做矩阵乘法（就是之前说的`q×k^T/√d_k`），得到 “我” 对 “我、爱、吃、苹果” 的原始分数（比如 [0.2, 0.5, 0.1, 0.2]）；
4.经过 Softmax 归一化，得到 “我” 对其他 token 的注意力权重（比如 [0.2, 0.5, 0.1, 0.2] 归一化后总和为 1）；
5.用这个权重对所有 token 的 V 向量（v1~v4）做加权求和，得到 “我” 的最终注意力输出（融合了 “我” 对其他所有 token 的关注）。
```
通过这样的机制，我们可以找到一段文本每一个token与其他所有token的相关关系大小，从而建模文本之间的依赖关系。
```
attention(x,x,x)
#attention为上文定义的注意力计算函数
```
---
### 2.1.5 掩码自注意力（Mask Self-Attention)
是指使用自注意力掩码的自注意力机制。
掩码的作用就是遮蔽一些特定位置的token，模型在学习过程中，会忽略掉被遮蔽的token。
为何要使用掩码 你有想过这个问题吗？
其实就是通过掩码能让模型只能使用历史信息进行预测而不能看到未来消息。
使用注意力机制的 Transformer 模型也是通过类似于 n-gram 的语言模型任务来学习的，也就是**对一个文本序列，不断根据之前的 token 来预测下一个 token，直到将整个文本序列补全。**
```
Step 1：输入 【BOS】，输出 I
Step 2：输入 【BOS】I，输出 like
Step 3：输入 【BOS】I like，输出 you
Step 4：输入 【BOS】I like you，输出 【EOS】
```
这个例子我们可以看出是串行计算
其实transformer有一个好处在于 相对于RNN的核心优势在于其**可以并行计算，具有更高的计算效率。**
这样可以看出来这个效率是很高的，所以transformer就提出了掩码自注意力机制。

观察上述的掩码，我们可以发现其实则是一个和文本序列等长的上三角矩阵。
我们可以简单地通过创建一个和输入同等长度的上三角矩阵作为注意力掩码，
再使用掩码来遮蔽掉输入即可。
也就是说，当输入维度为 （batch_size, seq_len, hidden_size）时，我们的 Mask 矩阵维度一般为 (1, seq_len, seq_len)（通过广播实现同一个 batch 中不同样本的计算）。

在具体实现中，我们可以通过以下代码生成Mask矩阵：
```
#创建一个上三角矩阵，用于遮蔽未来信息。
#先通过full函数创建一个1 * seq_len * seq_len的矩阵
mask = torch.full((1, args.max_seq_len, args.max_seq_len),float("-inf"))
#triu函数的功能是创建一个上三角矩阵
mask = torch.triu(mask, diagonal=1)
```
生成的 Mask 矩阵会是一个上三角矩阵，上三角位置的元素均为 -inf，其他位置的元素置为0。

在注意力计算时，我们会将计算得到的注意力分数与这个掩码做和，再进行 Softmax 操作：
```
#此处的score 为计算得到的注意力分数 ，mask为上文生成的掩码矩阵
scores = scores + mask[:, :seqlen, :seqlen]
scores = F.softmax(scores.float(),dim=-1).type_as(xq)
```
通过做求和，上三角区域（也就是应该被遮蔽的token对应的位置）的注意力分数都变成了 -inf，而下三角区域的分数不变。再做Softmax操作，-inf 的值在经过Softmax之后被置为0。
![[Pasted image 20250825215054.png]]
从而忽略了上三角区域计算的注意力分数，从而实现了注意力遮蔽。

---
## 2.1.6 多头注意力
注意力机制可以实现并行化与长期依赖关系拟合，但**一次注意力计算只能拟合一种相关关系**，单一的注意力机制很难全面拟合语句序列里的相关关系。
因此Transformer使用了多头注意力机制（Multi-Head Attention),即**同时对一个语料进行多次注意力计算，每次注意力计算都能拟合不同的关系，将最后的多次结果拼接起来作为最后的输出，即可全面深入地拟合语言信息。**

上层与下层分别是两个注意力头对同一段语句序列进行自注意力计算的结果，可以看到，对于不同的注意力头，能够拟合不同层次的相关信息。
**通过多个注意力头同时计算，能够更全面地拟合语句关系。**

事实上，所谓多头注意力机制的本质就是
将**原始的输入序列进行多组的自注意力处理，然后再将每一组得到的自注意力结果拼接起来，再通过一个线性层进行处理，得到最终的输出。**





----
## 2.2 Encoder -Decoder
Transformer的核心就是注意力机制
《Attention is all you Need》一文中，抛弃了传统的CNN、RNN的架构，使用的是注意力机制的两个核心组件--Encoder (编码器）和Decoder(解码器) 。
事实上，后续基于Transformer架构而来的预训练语言模型基本都是对Encoder-Decoder部分来进行改进来构建新的模型架构，
例如只使用Encoder的BERT 、只使用Decoder的GPT



---
### 2.2.1  Seq2Seq 模型
接下来我们以注意力机制为机制 来讲解下Transformer所针对的Seq2Seq任务出发，解析Transformer的 Encoder-Decoder 结构

Seq2Seq（Sequence to Sequence），即序列到序列，是一种经典的NLP任务。
具体而言，是指模型输入的是一个自然语言序列 input=(x1,x2,x3...xn)input=(x1​,x2​,x3​...xn​) ，输出的是一个可能不等长的自然语言序列 output=(y1,y2,y3...ym)output=(y1​,y2​,y3​...ym​) 。
事实上，这个是NLP最经典的任务，几乎所有的NLP任务都可以视为Seq2Seq任务

比如机器翻译啊，Transformer最开始就是应用在机器翻译任务上的。
对于这个任务来说 其实就是Encoder--- Decoder
**所谓编码，就是将输入的自然语言序列通过隐藏层编码成能够表征语义的向量（或矩阵），可以简单理解为更复杂的词向量表示。**
**而解码，就是对输入的自然语言序列编码得到的向量或矩阵通过隐藏层输出，再解码成对应的自然语言目标序列。**
![[Pasted image 20250826134642.png]]


question:为什么说encoder可以双向 什么正向看下可以 反向看下也可以
answer:Encoder核心在于理解序列，而非生成序列 任务的本质决定了它不需要 “隐瞒未来信息”，反而需要 “完整的上下文” 才能做好工作。
“双向” 指的是 Encoder 中的自注意力机制**允许每个 token 同时关注序列中 “前面的 token” 和 “后面的 token”**

- 编码器是没有输出的RNN
- 编码器最后时间步的隐状态用作编码器的初始隐状态，合并到输入
- ![[Pasted image 20250826133016.png]]
- 其实这个Seq2Seq有两种 
- 一种是训练时候 训练时解码器使用目标句子作为输入
- ![[Pasted image 20250826135059.png]]
- 推理就没有目标句子 所以 只能通过前一个token来预测下一个token



接下来我们来深入剖析下Encoder和Decoder内部**传统神经网络**的经典结构--
前馈神经网络（FNN）、层归一化（Layer Norm）和残差连接（Residual Connection）
然后进一步分析下Encoder和Decoder的内部结构

---
### 2.2.2 前馈神经网络
Feed Forward Neural Network(FNN)
这个也就是我们刚开始提到的**每一层神经元都和上下两层的每一个神经元完全连接的网络结构。**
**每一个Encoder Layer**都包含**一个上文讲的注意力机制**和**一个前馈神经网络**。

多层感知机（MLP）是一种前馈神经网络

注意，Transformer的前馈神经网络是由两个线性层加一个RELU激活函数组成的
前馈神经网络还加入了一个Dropout层来防止过拟合

---
### 2.2.3 层归一化
层归一化，也就是Layer Norm
神经网络主流的归一化一般有两种：**批归一化** 和 **层归一化**

归一化的核心是**为了让不同层输入的取值范围或者分布能够比较一致。** 
由于深度神经网络每一层的输入都是上一层的输出，因此多层传递下，对网络中较高的层，
之前的所有神经层的参数变化会导致其输入的分布发生较大的改变。
其实很好理解，也就是随着神经网络参数的更新，各层的输出分布是不相同的，且差异会随着网络深度的增大而增大。
但是，需要预测的条件分布始终是相同的，从而也就造成了预测的偏差。

因此，归一化显得尤其重要，将每一层的输入都归一化成标准的正态分布。
批归一化是指在一个mini-batch上进行归一化
![[Pasted image 20250826150627.png]]

但是，batch归一化有一些缺陷，例如：
- 当**显存有限，mini-batch 较小**时，Batch Norm 取的样本的均值和方差不能反映全局的统计分布信息，从而导致效果变差；**（不能以小盖全）**
- 对于在**时间维度**展开的 RNN，不同句子的同一分布大概率不同，所以 Batch Norm 的归一化会失去意义；
- 在训练时，Batch Norm 需要保存每个 step 的统计信息（均值和方差）。**在测试时，由于变长句子的特性，测试集可能出现比训练集更长的句子，所以对于后面位置的 step，是没有训练的统计量使用的；**
- 应用 Batch Norm，每个 step 都需要去保存和计算 batch 统计量，**耗时又耗力**
```
class LayerNorm(nn.Module):
    ''' Layer Norm 层'''
    def __init__(self, features, eps=1e-6):
    super().__init__()
    # 线性矩阵做映射
    self.a_2 = nn.Parameter(torch.ones(features))   初始化全为1的张量，用于缩放归一化后的值
    self.b_2 = nn.Parameter(torch.zeros(features))   初始化全为0的张量，用于偏移归一化后的值
    self.eps = eps                                     将传入的eps值保存在类的属性
    
    def forward(self, x):
    # 在统计每个样本所有维度的值，求均值和方差
    mean = x.mean(-1, keepdim=True) # mean: [bsz, max_len, 1]
    std = x.std(-1, keepdim=True) # std: [bsz, max_len, 1]
    # 注意这里也在最后一个维度发生了广播
    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
```
这个代码的过程其实就是上面那个公式 一个归一化
![[Pasted image 20250826152056.png]]

---
### 2.2.4 残差连接
由于Transformer模型结构较复杂、层数较深，
为了避免模型退化，采用了残差连接的思想来连接每一个子层。
理解：
残差连接，即下一层的输入不仅是上一层的输出还有上一层的输-入
残差连接允许最底层信息直接传到最高层，让高层专注于残差学习
**【残差连接的作用是将输入直接传递到输出，有助于缓解深层网络中的梯度消失问题。】**
Eg.
在Encoder中，在第一个子层，**输入进入多头自注意力层的同时会直接传递到该层的输出，然后该层的输出会与原输出相加，再进行标准化**。
其实很好理解 Decoder层里面有六个Decoder layer 然后每个Decoder layer层里面有两个：一个是多头注意力机制 一个是前馈神经网络 两个都需要残差处理
他们都是**先归一化处理 然后再经过相关的层（比如前者就是多头注意力层 后者就是前馈神经网络） 然后再进行残差处理**

我们在代码实现中，通过在层的forward计算中加上原值来实现残差连接：
```
#注意力计算
h = x + self.attention.forward(self.attention_norm(x))
#经过前馈神经网络
out = h + self.feed_forward(self.fnn_norm(h))
```


---
### 2.24 Encoder







----
## 2.3 搭建一个 Transformer
---
### 2.3.1 Embedding层
我们在之前讲过，在NLP 任务中，我们往往需要将自然语言的输入转化为机器可以处理的向量。
在深度学习中，承担这个任务的组件就是**Embedding层**。

Embedding层其实就是一个**存储固定大小的词典的嵌入向量查找表**。
也就是说，在输入神经网络之前，我们往往会先让自然语言输入通过分词器tokenizer
分词器的作用是**把自然语言输入切分成token并转化为一个固定的index**

例如，分词器设置为4
自然语言输入：“我喜欢你”
```
input: 我
output: 0

input: 喜欢
output: 1

input：你
output: 2
```
实际上很复杂，可以切分成词、子词、字符....
词表的大小则往往高达数万数十万。

因此，Embedding层的输入往往是一个（batch_size,seq_len,1)的矩阵，
第一个维度是一次批处理的数量
第二个维度是自然语言序列的长度，
第三个维度是token通过tokenizer转化为的index值
例如：
对于上述输入，Embedding层的输入是：
```
[[[0],[1],[2]]]
```
其 batch_size 为1，seq_len 为3，转化出来的 index 如上

而Embedding**内部**其实是一个**可训练的（Vocab_size, embedding_dim)的权重矩阵**，**词表里的每一个值，都对应一行维度为embedding_dim的向量。**
对于输入的值，会对应到这个词向量，然后拼接成（batch_size,seq_len,embedding_dim)的矩阵输出。
上述实现并不复杂，我们可以直接使用torch中的Embedding层：
```
self.tok_embedding = nn.Embedding(args.vocab_size, args.dim)
```


---
### 2.3.2 位置编码
注意力机制可以实现良好的并行计算
但同时，其注意力计算的方式也导致序列中相对位置的丢失，
在RNN 、LSTM 中，输入序列会沿着语句本身的顺序被依次递归处理，因此输入序列的顺序提供了极其重要的信息，这也和自然语言的本身特性非常吻合。

但从上文对注意力机制的分析我们可以发现，**在注意力机制的计算过程中，对于序列中的每一个 token，其他各个位置对其来说都是平等的**，即“我喜欢你”和“你喜欢我”在注意力机制看来是完全相同的，但无疑这是注意力机制存在的一个巨大问题。
因此，为使用序列顺序信息，保留序列中的相对位置信息，Transformer 采用了**位置编码机制**，该机制也在之后被多种模型沿用。

位置编码，
即**根据序列中token的相对位置对其进行编码，再将位置编码加入词向量编码中。**
位置编码的方式很多，Transformer使用了正余弦函数来进行位置编码（绝对位置编码Sinusoidal），其编码方式为：
![[Pasted image 20250826190430.png]]
解释：
pos为token在句子中的位置，2i 和 2i+1 则是指示了 token 是奇数位置还是偶数位置
![[Pasted image 20250826194850.png]]

代码：
```
import numpy as np
import matplotlib.pyplot as plt
def PositionEncoding(seq_len, d_model, n=10000):
    P = np.zeros((seq_len, d_model))
    for k in range(seq_len):
        for i in np.arange(int(d_model/2)):
            denominator = np.power(n, 2*i/d_model)
            P[k, 2*i] = np.sin(k/denominator)
            P[k, 2*i+1] = np.cos(k/denominator)
    return P

P = PositionEncoding(seq_len=4, d_model=4, n=100)
print(P)
``````
[[ 0.          1.          0.          1.        ]
 [ 0.84147098  0.54030231  0.09983342  0.99500417]
 [ 0.90929743 -0.41614684  0.19866933  0.98006658]
 [ 0.14112001 -0.9899925   0.29552021  0.95533649]]

---
### 2.3.3 一个完整的Transformer
paper里面的图是![[Pasted image 20250826194957.png]]
配图里面，LayerNorm层放在了Attention层后面，也就是“Post-Norm”结构，但在其发布的源代码中，LayerNorm层放在Attention层前面，也就是“Pre-Norm”。
考虑到目前 LLM 一般采用“Pre-Norm”结构（可以使 loss 更稳定），本文在实现时采用“Pre-Norm”结构。

如图，经过tokenier映射后的输出先经过Embedding层和Positional Embedding 层编码，
然后进入了N 个 Encoder 和 N 个 Decoder（在 Transformer 原模型中，N 取为6），
最后经过一个线性层和一个 Softmax 层就得到了最终输出。